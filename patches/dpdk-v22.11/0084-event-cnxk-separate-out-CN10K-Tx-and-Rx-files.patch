From 26e5fde0fea0efce69b5218a84f16eab52de51c7 Mon Sep 17 00:00:00 2001
From: Rahul Bhansali <rbhansali@marvell.com>
Date: Thu, 15 Dec 2022 12:18:21 +0530
Subject: [PATCH 084/955] event/cnxk: separate out CN10K Tx and Rx files

Separates Tx and Rx fastpath files to save recompilation
time for independent changes.

Signed-off-by: Rahul Bhansali <rbhansali@marvell.com>
Signed-off-by: Pavan Nikhilesh <pbhagavatula@marvell.com>
Change-Id: I7632f368eff39e429d8261c65ad7ab90e583a8d8
Reviewed-on: https://sj1git1.cavium.com/c/IP/SW/dataplane/dpdk/+/91559
Reviewed-by: Jerin Jacob Kollanukkaran <jerinj@marvell.com>
Tested-by: Jerin Jacob Kollanukkaran <jerinj@marvell.com>
Reviewed-on: https://sj1git1.cavium.com/c/IP/SW/dataplane/dpdk/+/92680
---
 drivers/event/cnxk/cn10k_eventdev.c          |   1 +
 drivers/event/cnxk/cn10k_tx_worker.h         | 253 +++++++++++++++
 drivers/event/cnxk/cn10k_worker.c            |  71 ++++
 drivers/event/cnxk/cn10k_worker.h            | 320 +------------------
 drivers/event/cnxk/tx/cn10k/tx_0_15.c        |   2 +-
 drivers/event/cnxk/tx/cn10k/tx_0_15_seg.c    |   2 +-
 drivers/event/cnxk/tx/cn10k/tx_112_127.c     |   2 +-
 drivers/event/cnxk/tx/cn10k/tx_112_127_seg.c |   2 +-
 drivers/event/cnxk/tx/cn10k/tx_16_31.c       |   2 +-
 drivers/event/cnxk/tx/cn10k/tx_16_31_seg.c   |   2 +-
 drivers/event/cnxk/tx/cn10k/tx_32_47.c       |   2 +-
 drivers/event/cnxk/tx/cn10k/tx_32_47_seg.c   |   2 +-
 drivers/event/cnxk/tx/cn10k/tx_48_63.c       |   2 +-
 drivers/event/cnxk/tx/cn10k/tx_48_63_seg.c   |   2 +-
 drivers/event/cnxk/tx/cn10k/tx_64_79.c       |   2 +-
 drivers/event/cnxk/tx/cn10k/tx_64_79_seg.c   |   2 +-
 drivers/event/cnxk/tx/cn10k/tx_80_95.c       |   2 +-
 drivers/event/cnxk/tx/cn10k/tx_80_95_seg.c   |   2 +-
 drivers/event/cnxk/tx/cn10k/tx_96_111.c      |   2 +-
 drivers/event/cnxk/tx/cn10k/tx_96_111_seg.c  |   2 +-
 20 files changed, 342 insertions(+), 335 deletions(-)
 create mode 100644 drivers/event/cnxk/cn10k_tx_worker.h

diff --git a/drivers/event/cnxk/cn10k_eventdev.c b/drivers/event/cnxk/cn10k_eventdev.c
index 1233a09159000..b6ddef9c2fa91 100644
--- a/drivers/event/cnxk/cn10k_eventdev.c
+++ b/drivers/event/cnxk/cn10k_eventdev.c
@@ -2,6 +2,7 @@
  * Copyright(C) 2021 Marvell.
  */
 
+#include "cn10k_tx_worker.h"
 #include "cn10k_worker.h"
 #include "cnxk_eventdev.h"
 #include "cnxk_worker.h"
diff --git a/drivers/event/cnxk/cn10k_tx_worker.h b/drivers/event/cnxk/cn10k_tx_worker.h
new file mode 100644
index 0000000000000..85c386793572a
--- /dev/null
+++ b/drivers/event/cnxk/cn10k_tx_worker.h
@@ -0,0 +1,253 @@
+/* SPDX-License-Identifier: BSD-3-Clause
+ * Copyright(C) 2022 Marvell.
+ */
+
+#ifndef __CN10K_TX_WORKER_H__
+#define __CN10K_TX_WORKER_H__
+
+#include "cn10k_ethdev.h"
+#include "cn10k_tx.h"
+#include "cnxk_eventdev.h"
+
+/* CN10K Tx event fastpath */
+
+static __rte_always_inline struct cn10k_eth_txq *
+cn10k_sso_hws_xtract_meta(struct rte_mbuf *m, const uint64_t *txq_data)
+{
+	return (struct cn10k_eth_txq
+			*)(txq_data[(txq_data[m->port] >> 48) +
+				    rte_event_eth_tx_adapter_txq_get(m)] &
+			   (BIT_ULL(48) - 1));
+}
+
+static __rte_always_inline void
+cn10k_sso_txq_fc_wait(const struct cn10k_eth_txq *txq)
+{
+	while ((uint64_t)txq->nb_sqb_bufs_adj <=
+	       __atomic_load_n(txq->fc_mem, __ATOMIC_RELAXED))
+		;
+}
+
+static __rte_always_inline int32_t
+cn10k_sso_sq_depth(const struct cn10k_eth_txq *txq)
+{
+	return (txq->nb_sqb_bufs_adj -
+		__atomic_load_n((int16_t *)txq->fc_mem, __ATOMIC_RELAXED))
+	       << txq->sqes_per_sqb_log2;
+}
+
+static __rte_always_inline uint16_t
+cn10k_sso_tx_one(struct cn10k_sso_hws *ws, struct rte_mbuf *m, uint64_t *cmd,
+		 uint16_t lmt_id, uintptr_t lmt_addr, uint8_t sched_type,
+		 const uint64_t *txq_data, const uint32_t flags)
+{
+	uint8_t lnum = 0, loff = 0, shft = 0;
+	uint16_t ref_cnt = m->refcnt;
+	struct cn10k_eth_txq *txq;
+	uintptr_t laddr;
+	uint16_t segdw;
+	uintptr_t pa;
+	bool sec;
+
+	txq = cn10k_sso_hws_xtract_meta(m, txq_data);
+	if (cn10k_sso_sq_depth(txq) <= 0)
+		return 0;
+
+	if (flags & NIX_TX_OFFLOAD_MBUF_NOFF_F && txq->tx_compl.ena)
+		handle_tx_completion_pkts(txq, 1, 1);
+
+	cn10k_nix_tx_skeleton(txq, cmd, flags, 0);
+	/* Perform header writes before barrier
+	 * for TSO
+	 */
+	if (flags & NIX_TX_OFFLOAD_TSO_F)
+		cn10k_nix_xmit_prepare_tso(m, flags);
+
+	cn10k_nix_xmit_prepare(txq, m, cmd, flags, txq->lso_tun_fmt, &sec,
+			       txq->mark_flag, txq->mark_fmt);
+
+	laddr = lmt_addr;
+	/* Prepare CPT instruction and get nixtx addr if
+	 * it is for CPT on same lmtline.
+	 */
+	if (flags & NIX_TX_OFFLOAD_SECURITY_F && sec)
+		cn10k_nix_prep_sec(m, cmd, &laddr, lmt_addr, &lnum, &loff,
+				   &shft, txq->sa_base, flags);
+
+	/* Move NIX desc to LMT/NIXTX area */
+	cn10k_nix_xmit_mv_lmt_base(laddr, cmd, flags);
+
+	if (flags & NIX_TX_MULTI_SEG_F)
+		segdw = cn10k_nix_prepare_mseg(txq, m, (uint64_t *)laddr, flags);
+	else
+		segdw = cn10k_nix_tx_ext_subs(flags) + 2;
+
+	cn10k_nix_xmit_prepare_tstamp(txq, laddr, m->ol_flags, segdw, flags);
+	if (flags & NIX_TX_OFFLOAD_SECURITY_F && sec)
+		pa = txq->cpt_io_addr | 3 << 4;
+	else
+		pa = txq->io_addr | ((segdw - 1) << 4);
+
+	if (!CNXK_TAG_IS_HEAD(ws->gw_rdata) && !sched_type)
+		ws->gw_rdata = roc_sso_hws_head_wait(ws->base);
+
+	cn10k_sso_txq_fc_wait(txq);
+	if (flags & NIX_TX_OFFLOAD_SECURITY_F && sec)
+		cn10k_nix_sec_fc_wait_one(txq);
+
+	roc_lmt_submit_steorl(lmt_id, pa);
+
+	if (flags & NIX_TX_OFFLOAD_MBUF_NOFF_F) {
+		if (ref_cnt > 1)
+			rte_io_wmb();
+	}
+	return 1;
+}
+
+static __rte_always_inline uint16_t
+cn10k_sso_vwqe_split_tx(struct cn10k_sso_hws *ws, struct rte_mbuf **mbufs,
+			uint16_t nb_mbufs, uint64_t *cmd,
+			const uint64_t *txq_data, const uint32_t flags)
+{
+	uint16_t count = 0, port, queue, ret = 0, last_idx = 0;
+	struct cn10k_eth_txq *txq;
+	int32_t space;
+	int i;
+
+	port = mbufs[0]->port;
+	queue = rte_event_eth_tx_adapter_txq_get(mbufs[0]);
+	for (i = 0; i < nb_mbufs; i++) {
+		if (port != mbufs[i]->port ||
+		    queue != rte_event_eth_tx_adapter_txq_get(mbufs[i])) {
+			if (count) {
+				txq = (struct cn10k_eth_txq
+					       *)(txq_data[(txq_data[port] >>
+							    48) +
+							   queue] &
+						  (BIT_ULL(48) - 1));
+				/* Transmit based on queue depth */
+				space = cn10k_sso_sq_depth(txq);
+				if (space < count)
+					goto done;
+				cn10k_nix_xmit_pkts_vector(
+					txq, (uint64_t *)ws, &mbufs[last_idx],
+					count, cmd, flags | NIX_TX_VWQE_F);
+				ret += count;
+				count = 0;
+			}
+			port = mbufs[i]->port;
+			queue = rte_event_eth_tx_adapter_txq_get(mbufs[i]);
+			last_idx = i;
+		}
+		count++;
+	}
+	if (count) {
+		txq = (struct cn10k_eth_txq
+			       *)(txq_data[(txq_data[port] >> 48) + queue] &
+				  (BIT_ULL(48) - 1));
+		/* Transmit based on queue depth */
+		space = cn10k_sso_sq_depth(txq);
+		if (space < count)
+			goto done;
+		cn10k_nix_xmit_pkts_vector(txq, (uint64_t *)ws,
+					   &mbufs[last_idx], count, cmd,
+					   flags | NIX_TX_VWQE_F);
+		ret += count;
+	}
+done:
+	return ret;
+}
+
+static __rte_always_inline uint16_t
+cn10k_sso_hws_event_tx(struct cn10k_sso_hws *ws, struct rte_event *ev,
+		       uint64_t *cmd, const uint64_t *txq_data,
+		       const uint32_t flags)
+{
+	struct cn10k_eth_txq *txq;
+	struct rte_mbuf *m;
+	uintptr_t lmt_addr;
+	uint16_t lmt_id;
+
+	lmt_addr = ws->lmt_base;
+	ROC_LMT_BASE_ID_GET(lmt_addr, lmt_id);
+
+	if (ev->event_type & RTE_EVENT_TYPE_VECTOR) {
+		struct rte_mbuf **mbufs = ev->vec->mbufs;
+		uint64_t meta = *(uint64_t *)ev->vec;
+		uint16_t offset, nb_pkts, left;
+		int32_t space;
+
+		nb_pkts = meta & 0xFFFF;
+		offset = (meta >> 16) & 0xFFF;
+		if (meta & BIT(31)) {
+			txq = (struct cn10k_eth_txq
+				       *)(txq_data[(txq_data[meta >> 32] >>
+						    48) +
+						   (meta >> 48)] &
+					  (BIT_ULL(48) - 1));
+
+			/* Transmit based on queue depth */
+			space = cn10k_sso_sq_depth(txq);
+			if (space <= 0)
+				return 0;
+			nb_pkts = nb_pkts < space ? nb_pkts : (uint16_t)space;
+			cn10k_nix_xmit_pkts_vector(txq, (uint64_t *)ws,
+						   mbufs + offset, nb_pkts, cmd,
+						   flags | NIX_TX_VWQE_F);
+		} else {
+			nb_pkts = cn10k_sso_vwqe_split_tx(ws, mbufs + offset,
+							  nb_pkts, cmd,
+							  txq_data, flags);
+		}
+		left = (meta & 0xFFFF) - nb_pkts;
+
+		if (!left) {
+			rte_mempool_put(rte_mempool_from_obj(ev->vec), ev->vec);
+		} else {
+			*(uint64_t *)ev->vec =
+				(meta & ~0xFFFFFFFUL) |
+				(((uint32_t)nb_pkts + offset) << 16) | left;
+		}
+		rte_prefetch0(ws);
+		return !left;
+	}
+
+	m = ev->mbuf;
+	return cn10k_sso_tx_one(ws, m, cmd, lmt_id, lmt_addr, ev->sched_type,
+				txq_data, flags);
+}
+
+#define T(name, sz, flags)                                                     \
+	uint16_t __rte_hot cn10k_sso_hws_tx_adptr_enq_##name(                  \
+		void *port, struct rte_event ev[], uint16_t nb_events);        \
+	uint16_t __rte_hot cn10k_sso_hws_tx_adptr_enq_seg_##name(              \
+		void *port, struct rte_event ev[], uint16_t nb_events);
+
+NIX_TX_FASTPATH_MODES
+#undef T
+
+#define SSO_TX(fn, sz, flags)                                                  \
+	uint16_t __rte_hot fn(void *port, struct rte_event ev[],               \
+			      uint16_t nb_events)                              \
+	{                                                                      \
+		struct cn10k_sso_hws *ws = port;                               \
+		uint64_t cmd[sz];                                              \
+		RTE_SET_USED(nb_events);                                       \
+		return cn10k_sso_hws_event_tx(                                 \
+			ws, &ev[0], cmd, (const uint64_t *)ws->tx_adptr_data,  \
+			flags);                                                \
+	}
+
+#define SSO_TX_SEG(fn, sz, flags)                                              \
+	uint16_t __rte_hot fn(void *port, struct rte_event ev[],               \
+			      uint16_t nb_events)                              \
+	{                                                                      \
+		uint64_t cmd[(sz) + CNXK_NIX_TX_MSEG_SG_DWORDS - 2];           \
+		struct cn10k_sso_hws *ws = port;                               \
+		RTE_SET_USED(nb_events);                                       \
+		return cn10k_sso_hws_event_tx(                                 \
+			ws, &ev[0], cmd, (const uint64_t *)ws->tx_adptr_data,  \
+			(flags) | NIX_TX_MULTI_SEG_F);                         \
+	}
+
+#endif
diff --git a/drivers/event/cnxk/cn10k_worker.c b/drivers/event/cnxk/cn10k_worker.c
index 4581c412333c0..562d2fca13ba1 100644
--- a/drivers/event/cnxk/cn10k_worker.c
+++ b/drivers/event/cnxk/cn10k_worker.c
@@ -6,6 +6,77 @@
 #include "cnxk_eventdev.h"
 #include "cnxk_worker.h"
 
+/* SSO Operations */
+
+static __rte_always_inline uint8_t
+cn10k_sso_hws_new_event(struct cn10k_sso_hws *ws, const struct rte_event *ev)
+{
+	const uint32_t tag = (uint32_t)ev->event;
+	const uint8_t new_tt = ev->sched_type;
+	const uint64_t event_ptr = ev->u64;
+	const uint16_t grp = ev->queue_id;
+
+	rte_atomic_thread_fence(__ATOMIC_ACQ_REL);
+	if (ws->xaq_lmt <= *ws->fc_mem)
+		return 0;
+
+	cnxk_sso_hws_add_work(event_ptr, tag, new_tt, ws->grp_base + (grp << 12));
+	return 1;
+}
+
+static __rte_always_inline void
+cn10k_sso_hws_fwd_swtag(struct cn10k_sso_hws *ws, const struct rte_event *ev)
+{
+	const uint32_t tag = (uint32_t)ev->event;
+	const uint8_t new_tt = ev->sched_type;
+	const uint8_t cur_tt = CNXK_TT_FROM_TAG(ws->gw_rdata);
+
+	/* CNXK model
+	 * cur_tt/new_tt     SSO_TT_ORDERED SSO_TT_ATOMIC SSO_TT_UNTAGGED
+	 *
+	 * SSO_TT_ORDERED        norm           norm             untag
+	 * SSO_TT_ATOMIC         norm           norm		   untag
+	 * SSO_TT_UNTAGGED       norm           norm             NOOP
+	 */
+
+	if (new_tt == SSO_TT_UNTAGGED) {
+		if (cur_tt != SSO_TT_UNTAGGED)
+			cnxk_sso_hws_swtag_untag(ws->base + SSOW_LF_GWS_OP_SWTAG_UNTAG);
+	} else {
+		cnxk_sso_hws_swtag_norm(tag, new_tt, ws->base + SSOW_LF_GWS_OP_SWTAG_NORM);
+	}
+	ws->swtag_req = 1;
+}
+
+static __rte_always_inline void
+cn10k_sso_hws_fwd_group(struct cn10k_sso_hws *ws, const struct rte_event *ev,
+			const uint16_t grp)
+{
+	const uint32_t tag = (uint32_t)ev->event;
+	const uint8_t new_tt = ev->sched_type;
+
+	plt_write64(ev->u64, ws->base + SSOW_LF_GWS_OP_UPD_WQP_GRP1);
+	cnxk_sso_hws_swtag_desched(tag, new_tt, grp, ws->base + SSOW_LF_GWS_OP_SWTAG_DESCHED);
+}
+
+static __rte_always_inline void
+cn10k_sso_hws_forward_event(struct cn10k_sso_hws *ws,
+			    const struct rte_event *ev)
+{
+	const uint8_t grp = ev->queue_id;
+
+	/* Group hasn't changed, Use SWTAG to forward the event */
+	if (CNXK_GRP_FROM_TAG(ws->gw_rdata) == grp)
+		cn10k_sso_hws_fwd_swtag(ws, ev);
+	else
+		/*
+		 * Group has been changed for group based work pipelining,
+		 * Use deschedule/add_work operation to transfer the event to
+		 * new group/core
+		 */
+		cn10k_sso_hws_fwd_group(ws, ev, grp);
+}
+
 uint16_t __rte_hot
 cn10k_sso_hws_enq(void *port, const struct rte_event *ev)
 {
diff --git a/drivers/event/cnxk/cn10k_worker.h b/drivers/event/cnxk/cn10k_worker.h
index 332a2e27c205f..79b76869ff2e6 100644
--- a/drivers/event/cnxk/cn10k_worker.h
+++ b/drivers/event/cnxk/cn10k_worker.h
@@ -5,91 +5,12 @@
 #ifndef __CN10K_WORKER_H__
 #define __CN10K_WORKER_H__
 
-#include <rte_vect.h>
-
 #include "cn10k_cryptodev_ops.h"
-#include "cnxk_ethdev.h"
-#include "cnxk_eventdev.h"
 #include "cnxk_worker.h"
-
 #include "cn10k_ethdev.h"
 #include "cn10k_rx.h"
-#include "cn10k_tx.h"
-
-/* SSO Operations */
 
-static __rte_always_inline uint8_t
-cn10k_sso_hws_new_event(struct cn10k_sso_hws *ws, const struct rte_event *ev)
-{
-	const uint32_t tag = (uint32_t)ev->event;
-	const uint8_t new_tt = ev->sched_type;
-	const uint64_t event_ptr = ev->u64;
-	const uint16_t grp = ev->queue_id;
-
-	rte_atomic_thread_fence(__ATOMIC_ACQ_REL);
-	if (ws->xaq_lmt <= *ws->fc_mem)
-		return 0;
-
-	cnxk_sso_hws_add_work(event_ptr, tag, new_tt,
-			      ws->grp_base + (grp << 12));
-	return 1;
-}
-
-static __rte_always_inline void
-cn10k_sso_hws_fwd_swtag(struct cn10k_sso_hws *ws, const struct rte_event *ev)
-{
-	const uint32_t tag = (uint32_t)ev->event;
-	const uint8_t new_tt = ev->sched_type;
-	const uint8_t cur_tt = CNXK_TT_FROM_TAG(ws->gw_rdata);
-
-	/* CNXK model
-	 * cur_tt/new_tt     SSO_TT_ORDERED SSO_TT_ATOMIC SSO_TT_UNTAGGED
-	 *
-	 * SSO_TT_ORDERED        norm           norm             untag
-	 * SSO_TT_ATOMIC         norm           norm		   untag
-	 * SSO_TT_UNTAGGED       norm           norm             NOOP
-	 */
-
-	if (new_tt == SSO_TT_UNTAGGED) {
-		if (cur_tt != SSO_TT_UNTAGGED)
-			cnxk_sso_hws_swtag_untag(ws->base +
-						 SSOW_LF_GWS_OP_SWTAG_UNTAG);
-	} else {
-		cnxk_sso_hws_swtag_norm(tag, new_tt,
-					ws->base + SSOW_LF_GWS_OP_SWTAG_NORM);
-	}
-	ws->swtag_req = 1;
-}
-
-static __rte_always_inline void
-cn10k_sso_hws_fwd_group(struct cn10k_sso_hws *ws, const struct rte_event *ev,
-			const uint16_t grp)
-{
-	const uint32_t tag = (uint32_t)ev->event;
-	const uint8_t new_tt = ev->sched_type;
-
-	plt_write64(ev->u64, ws->base + SSOW_LF_GWS_OP_UPD_WQP_GRP1);
-	cnxk_sso_hws_swtag_desched(tag, new_tt, grp,
-				   ws->base + SSOW_LF_GWS_OP_SWTAG_DESCHED);
-}
-
-static __rte_always_inline void
-cn10k_sso_hws_forward_event(struct cn10k_sso_hws *ws,
-			    const struct rte_event *ev)
-{
-	const uint8_t grp = ev->queue_id;
-
-	/* Group hasn't changed, Use SWTAG to forward the event */
-	if (CNXK_GRP_FROM_TAG(ws->gw_rdata) == grp)
-		cn10k_sso_hws_fwd_swtag(ws, ev);
-	else
-		/*
-		 * Group has been changed for group based work pipelining,
-		 * Use deschedule/add_work operation to transfer the event to
-		 * new group/core
-		 */
-		cn10k_sso_hws_fwd_group(ws, ev, grp);
-}
+/* CN10K Rx event fastpath */
 
 static __rte_always_inline void
 cn10k_wqe_to_mbuf(uint64_t wqe, const uint64_t __mbuf, uint8_t port_id,
@@ -517,243 +438,4 @@ NIX_RX_FASTPATH_MODES
 		return fn(port, ev, timeout_ticks);                            \
 	}
 
-static __rte_always_inline struct cn10k_eth_txq *
-cn10k_sso_hws_xtract_meta(struct rte_mbuf *m, const uint64_t *txq_data)
-{
-	return (struct cn10k_eth_txq
-			*)(txq_data[(txq_data[m->port] >> 48) +
-				    rte_event_eth_tx_adapter_txq_get(m)] &
-			   (BIT_ULL(48) - 1));
-}
-
-static __rte_always_inline void
-cn10k_sso_txq_fc_wait(const struct cn10k_eth_txq *txq)
-{
-	while ((uint64_t)txq->nb_sqb_bufs_adj <=
-	       __atomic_load_n(txq->fc_mem, __ATOMIC_RELAXED))
-		;
-}
-
-static __rte_always_inline int32_t
-cn10k_sso_sq_depth(const struct cn10k_eth_txq *txq)
-{
-	return (txq->nb_sqb_bufs_adj -
-		__atomic_load_n((int16_t *)txq->fc_mem, __ATOMIC_RELAXED))
-	       << txq->sqes_per_sqb_log2;
-}
-
-static __rte_always_inline uint16_t
-cn10k_sso_tx_one(struct cn10k_sso_hws *ws, struct rte_mbuf *m, uint64_t *cmd,
-		 uint16_t lmt_id, uintptr_t lmt_addr, uint8_t sched_type,
-		 const uint64_t *txq_data, const uint32_t flags)
-{
-	uint8_t lnum = 0, loff = 0, shft = 0;
-	uint16_t ref_cnt = m->refcnt;
-	struct cn10k_eth_txq *txq;
-	uintptr_t laddr;
-	uint16_t segdw;
-	uintptr_t pa;
-	bool sec;
-
-	txq = cn10k_sso_hws_xtract_meta(m, txq_data);
-	if (cn10k_sso_sq_depth(txq) <= 0)
-		return 0;
-
-	if (flags & NIX_TX_OFFLOAD_MBUF_NOFF_F && txq->tx_compl.ena)
-		handle_tx_completion_pkts(txq, 1, 1);
-
-	cn10k_nix_tx_skeleton(txq, cmd, flags, 0);
-	/* Perform header writes before barrier
-	 * for TSO
-	 */
-	if (flags & NIX_TX_OFFLOAD_TSO_F)
-		cn10k_nix_xmit_prepare_tso(m, flags);
-
-	cn10k_nix_xmit_prepare(txq, m, cmd, flags, txq->lso_tun_fmt, &sec,
-			       txq->mark_flag, txq->mark_fmt);
-
-	laddr = lmt_addr;
-	/* Prepare CPT instruction and get nixtx addr if
-	 * it is for CPT on same lmtline.
-	 */
-	if (flags & NIX_TX_OFFLOAD_SECURITY_F && sec)
-		cn10k_nix_prep_sec(m, cmd, &laddr, lmt_addr, &lnum, &loff,
-				   &shft, txq->sa_base, flags);
-
-	/* Move NIX desc to LMT/NIXTX area */
-	cn10k_nix_xmit_mv_lmt_base(laddr, cmd, flags);
-
-	if (flags & NIX_TX_MULTI_SEG_F)
-		segdw = cn10k_nix_prepare_mseg(txq, m, (uint64_t *)laddr, flags);
-	else
-		segdw = cn10k_nix_tx_ext_subs(flags) + 2;
-
-	cn10k_nix_xmit_prepare_tstamp(txq, laddr, m->ol_flags, segdw, flags);
-	if (flags & NIX_TX_OFFLOAD_SECURITY_F && sec)
-		pa = txq->cpt_io_addr | 3 << 4;
-	else
-		pa = txq->io_addr | ((segdw - 1) << 4);
-
-	if (!CNXK_TAG_IS_HEAD(ws->gw_rdata) && !sched_type)
-		ws->gw_rdata = roc_sso_hws_head_wait(ws->base);
-
-	cn10k_sso_txq_fc_wait(txq);
-	if (flags & NIX_TX_OFFLOAD_SECURITY_F && sec)
-		cn10k_nix_sec_fc_wait_one(txq);
-
-	roc_lmt_submit_steorl(lmt_id, pa);
-
-	if (flags & NIX_TX_OFFLOAD_MBUF_NOFF_F) {
-		if (ref_cnt > 1)
-			rte_io_wmb();
-	}
-	return 1;
-}
-
-static __rte_always_inline uint16_t
-cn10k_sso_vwqe_split_tx(struct cn10k_sso_hws *ws, struct rte_mbuf **mbufs,
-			uint16_t nb_mbufs, uint64_t *cmd,
-			const uint64_t *txq_data, const uint32_t flags)
-{
-	uint16_t count = 0, port, queue, ret = 0, last_idx = 0;
-	struct cn10k_eth_txq *txq;
-	int32_t space;
-	int i;
-
-	port = mbufs[0]->port;
-	queue = rte_event_eth_tx_adapter_txq_get(mbufs[0]);
-	for (i = 0; i < nb_mbufs; i++) {
-		if (port != mbufs[i]->port ||
-		    queue != rte_event_eth_tx_adapter_txq_get(mbufs[i])) {
-			if (count) {
-				txq = (struct cn10k_eth_txq
-					       *)(txq_data[(txq_data[port] >>
-							    48) +
-							   queue] &
-						  (BIT_ULL(48) - 1));
-				/* Transmit based on queue depth */
-				space = cn10k_sso_sq_depth(txq);
-				if (space < count)
-					goto done;
-				cn10k_nix_xmit_pkts_vector(
-					txq, (uint64_t *)ws, &mbufs[last_idx],
-					count, cmd, flags | NIX_TX_VWQE_F);
-				ret += count;
-				count = 0;
-			}
-			port = mbufs[i]->port;
-			queue = rte_event_eth_tx_adapter_txq_get(mbufs[i]);
-			last_idx = i;
-		}
-		count++;
-	}
-	if (count) {
-		txq = (struct cn10k_eth_txq
-			       *)(txq_data[(txq_data[port] >> 48) + queue] &
-				  (BIT_ULL(48) - 1));
-		/* Transmit based on queue depth */
-		space = cn10k_sso_sq_depth(txq);
-		if (space < count)
-			goto done;
-		cn10k_nix_xmit_pkts_vector(txq, (uint64_t *)ws,
-					   &mbufs[last_idx], count, cmd,
-					   flags | NIX_TX_VWQE_F);
-		ret += count;
-	}
-done:
-	return ret;
-}
-
-static __rte_always_inline uint16_t
-cn10k_sso_hws_event_tx(struct cn10k_sso_hws *ws, struct rte_event *ev,
-		       uint64_t *cmd, const uint64_t *txq_data,
-		       const uint32_t flags)
-{
-	struct cn10k_eth_txq *txq;
-	struct rte_mbuf *m;
-	uintptr_t lmt_addr;
-	uint16_t lmt_id;
-
-	lmt_addr = ws->lmt_base;
-	ROC_LMT_BASE_ID_GET(lmt_addr, lmt_id);
-
-	if (ev->event_type & RTE_EVENT_TYPE_VECTOR) {
-		struct rte_mbuf **mbufs = ev->vec->mbufs;
-		uint64_t meta = *(uint64_t *)ev->vec;
-		uint16_t offset, nb_pkts, left;
-		int32_t space;
-
-		nb_pkts = meta & 0xFFFF;
-		offset = (meta >> 16) & 0xFFF;
-		if (meta & BIT(31)) {
-			txq = (struct cn10k_eth_txq
-				       *)(txq_data[(txq_data[meta >> 32] >>
-						    48) +
-						   (meta >> 48)] &
-					  (BIT_ULL(48) - 1));
-
-			/* Transmit based on queue depth */
-			space = cn10k_sso_sq_depth(txq);
-			if (space <= 0)
-				return 0;
-			nb_pkts = nb_pkts < space ? nb_pkts : (uint16_t)space;
-			cn10k_nix_xmit_pkts_vector(txq, (uint64_t *)ws,
-						   mbufs + offset, nb_pkts, cmd,
-						   flags | NIX_TX_VWQE_F);
-		} else {
-			nb_pkts = cn10k_sso_vwqe_split_tx(ws, mbufs + offset,
-							  nb_pkts, cmd,
-							  txq_data, flags);
-		}
-		left = (meta & 0xFFFF) - nb_pkts;
-
-		if (!left) {
-			rte_mempool_put(rte_mempool_from_obj(ev->vec), ev->vec);
-		} else {
-			*(uint64_t *)ev->vec =
-				(meta & ~0xFFFFFFFUL) |
-				(((uint32_t)nb_pkts + offset) << 16) | left;
-		}
-		rte_prefetch0(ws);
-		return !left;
-	}
-
-	m = ev->mbuf;
-	return cn10k_sso_tx_one(ws, m, cmd, lmt_id, lmt_addr, ev->sched_type,
-				txq_data, flags);
-}
-
-#define T(name, sz, flags)                                                     \
-	uint16_t __rte_hot cn10k_sso_hws_tx_adptr_enq_##name(                  \
-		void *port, struct rte_event ev[], uint16_t nb_events);        \
-	uint16_t __rte_hot cn10k_sso_hws_tx_adptr_enq_seg_##name(              \
-		void *port, struct rte_event ev[], uint16_t nb_events);
-
-NIX_TX_FASTPATH_MODES
-#undef T
-
-#define SSO_TX(fn, sz, flags)                                                  \
-	uint16_t __rte_hot fn(void *port, struct rte_event ev[],               \
-			      uint16_t nb_events)                              \
-	{                                                                      \
-		struct cn10k_sso_hws *ws = port;                               \
-		uint64_t cmd[sz];                                              \
-		RTE_SET_USED(nb_events);                                       \
-		return cn10k_sso_hws_event_tx(                                 \
-			ws, &ev[0], cmd, (const uint64_t *)ws->tx_adptr_data,  \
-			flags);                                                \
-	}
-
-#define SSO_TX_SEG(fn, sz, flags)                                              \
-	uint16_t __rte_hot fn(void *port, struct rte_event ev[],               \
-			      uint16_t nb_events)                              \
-	{                                                                      \
-		uint64_t cmd[(sz) + CNXK_NIX_TX_MSEG_SG_DWORDS - 2];           \
-		struct cn10k_sso_hws *ws = port;                               \
-		RTE_SET_USED(nb_events);                                       \
-		return cn10k_sso_hws_event_tx(                                 \
-			ws, &ev[0], cmd, (const uint64_t *)ws->tx_adptr_data,  \
-			(flags) | NIX_TX_MULTI_SEG_F);                         \
-	}
-
 #endif
diff --git a/drivers/event/cnxk/tx/cn10k/tx_0_15.c b/drivers/event/cnxk/tx/cn10k/tx_0_15.c
index bae3a62eac686..e52e0d7c4cbb5 100644
--- a/drivers/event/cnxk/tx/cn10k/tx_0_15.c
+++ b/drivers/event/cnxk/tx/cn10k/tx_0_15.c
@@ -2,7 +2,7 @@
  * Copyright(C) 2022 Marvell.
  */
 
-#include "cn10k_worker.h"
+#include "cn10k_tx_worker.h"
 
 #define T(name, sz, flags) SSO_TX(cn10k_sso_hws_tx_adptr_enq_##name, sz, flags)
 
diff --git a/drivers/event/cnxk/tx/cn10k/tx_0_15_seg.c b/drivers/event/cnxk/tx/cn10k/tx_0_15_seg.c
index 0ffd0e50f77a6..87e588607c4d1 100644
--- a/drivers/event/cnxk/tx/cn10k/tx_0_15_seg.c
+++ b/drivers/event/cnxk/tx/cn10k/tx_0_15_seg.c
@@ -2,7 +2,7 @@
  * Copyright(C) 2022 Marvell.
  */
 
-#include "cn10k_worker.h"
+#include "cn10k_tx_worker.h"
 
 #define T(name, sz, flags)                                                     \
 	SSO_TX_SEG(cn10k_sso_hws_tx_adptr_enq_seg_##name, sz, flags)
diff --git a/drivers/event/cnxk/tx/cn10k/tx_112_127.c b/drivers/event/cnxk/tx/cn10k/tx_112_127.c
index 02c9ab1ae1e8d..b8129ed6383c3 100644
--- a/drivers/event/cnxk/tx/cn10k/tx_112_127.c
+++ b/drivers/event/cnxk/tx/cn10k/tx_112_127.c
@@ -2,7 +2,7 @@
  * Copyright(C) 2022 Marvell.
  */
 
-#include "cn10k_worker.h"
+#include "cn10k_tx_worker.h"
 
 #define T(name, sz, flags) SSO_TX(cn10k_sso_hws_tx_adptr_enq_##name, sz, flags)
 
diff --git a/drivers/event/cnxk/tx/cn10k/tx_112_127_seg.c b/drivers/event/cnxk/tx/cn10k/tx_112_127_seg.c
index b0abd8b328d8d..4c2786b0ad8d7 100644
--- a/drivers/event/cnxk/tx/cn10k/tx_112_127_seg.c
+++ b/drivers/event/cnxk/tx/cn10k/tx_112_127_seg.c
@@ -2,7 +2,7 @@
  * Copyright(C) 2022 Marvell.
  */
 
-#include "cn10k_worker.h"
+#include "cn10k_tx_worker.h"
 
 #define T(name, sz, flags)                                                     \
 	SSO_TX_SEG(cn10k_sso_hws_tx_adptr_enq_seg_##name, sz, flags)
diff --git a/drivers/event/cnxk/tx/cn10k/tx_16_31.c b/drivers/event/cnxk/tx/cn10k/tx_16_31.c
index cd6b1b56f8941..1d066954f049b 100644
--- a/drivers/event/cnxk/tx/cn10k/tx_16_31.c
+++ b/drivers/event/cnxk/tx/cn10k/tx_16_31.c
@@ -2,7 +2,7 @@
  * Copyright(C) 2022 Marvell.
  */
 
-#include "cn10k_worker.h"
+#include "cn10k_tx_worker.h"
 
 #define T(name, sz, flags) SSO_TX(cn10k_sso_hws_tx_adptr_enq_##name, sz, flags)
 
diff --git a/drivers/event/cnxk/tx/cn10k/tx_16_31_seg.c b/drivers/event/cnxk/tx/cn10k/tx_16_31_seg.c
index 52cbe5932f31c..d85d2ccd55073 100644
--- a/drivers/event/cnxk/tx/cn10k/tx_16_31_seg.c
+++ b/drivers/event/cnxk/tx/cn10k/tx_16_31_seg.c
@@ -2,7 +2,7 @@
  * Copyright(C) 2022 Marvell.
  */
 
-#include "cn10k_worker.h"
+#include "cn10k_tx_worker.h"
 
 #define T(name, sz, flags)                                                     \
 	SSO_TX_SEG(cn10k_sso_hws_tx_adptr_enq_seg_##name, sz, flags)
diff --git a/drivers/event/cnxk/tx/cn10k/tx_32_47.c b/drivers/event/cnxk/tx/cn10k/tx_32_47.c
index 5dd3aad0f31f7..3d9b590443875 100644
--- a/drivers/event/cnxk/tx/cn10k/tx_32_47.c
+++ b/drivers/event/cnxk/tx/cn10k/tx_32_47.c
@@ -2,7 +2,7 @@
  * Copyright(C) 2022 Marvell.
  */
 
-#include "cn10k_worker.h"
+#include "cn10k_tx_worker.h"
 
 #define T(name, sz, flags) SSO_TX(cn10k_sso_hws_tx_adptr_enq_##name, sz, flags)
 
diff --git a/drivers/event/cnxk/tx/cn10k/tx_32_47_seg.c b/drivers/event/cnxk/tx/cn10k/tx_32_47_seg.c
index a8e73445fffb1..4eaad2f8a19a9 100644
--- a/drivers/event/cnxk/tx/cn10k/tx_32_47_seg.c
+++ b/drivers/event/cnxk/tx/cn10k/tx_32_47_seg.c
@@ -2,7 +2,7 @@
  * Copyright(C) 2022 Marvell.
  */
 
-#include "cn10k_worker.h"
+#include "cn10k_tx_worker.h"
 
 #define T(name, sz, flags)                                                     \
 	SSO_TX_SEG(cn10k_sso_hws_tx_adptr_enq_seg_##name, sz, flags)
diff --git a/drivers/event/cnxk/tx/cn10k/tx_48_63.c b/drivers/event/cnxk/tx/cn10k/tx_48_63.c
index 8cd5c663d7b2a..eab139996838a 100644
--- a/drivers/event/cnxk/tx/cn10k/tx_48_63.c
+++ b/drivers/event/cnxk/tx/cn10k/tx_48_63.c
@@ -2,7 +2,7 @@
  * Copyright(C) 2022 Marvell.
  */
 
-#include "cn10k_worker.h"
+#include "cn10k_tx_worker.h"
 
 #define T(name, sz, flags) SSO_TX(cn10k_sso_hws_tx_adptr_enq_##name, sz, flags)
 
diff --git a/drivers/event/cnxk/tx/cn10k/tx_48_63_seg.c b/drivers/event/cnxk/tx/cn10k/tx_48_63_seg.c
index 71efca94781be..3b54242f613eb 100644
--- a/drivers/event/cnxk/tx/cn10k/tx_48_63_seg.c
+++ b/drivers/event/cnxk/tx/cn10k/tx_48_63_seg.c
@@ -2,7 +2,7 @@
  * Copyright(C) 2022 Marvell.
  */
 
-#include "cn10k_worker.h"
+#include "cn10k_tx_worker.h"
 
 #define T(name, sz, flags)                                                     \
 	SSO_TX_SEG(cn10k_sso_hws_tx_adptr_enq_seg_##name, sz, flags)
diff --git a/drivers/event/cnxk/tx/cn10k/tx_64_79.c b/drivers/event/cnxk/tx/cn10k/tx_64_79.c
index 36aea254e58cc..cbeebc1b77a8f 100644
--- a/drivers/event/cnxk/tx/cn10k/tx_64_79.c
+++ b/drivers/event/cnxk/tx/cn10k/tx_64_79.c
@@ -2,7 +2,7 @@
  * Copyright(C) 2022 Marvell.
  */
 
-#include "cn10k_worker.h"
+#include "cn10k_tx_worker.h"
 
 #define T(name, sz, flags) SSO_TX(cn10k_sso_hws_tx_adptr_enq_##name, sz, flags)
 
diff --git a/drivers/event/cnxk/tx/cn10k/tx_64_79_seg.c b/drivers/event/cnxk/tx/cn10k/tx_64_79_seg.c
index 918f9b6d3b7ad..34ddd6193c85e 100644
--- a/drivers/event/cnxk/tx/cn10k/tx_64_79_seg.c
+++ b/drivers/event/cnxk/tx/cn10k/tx_64_79_seg.c
@@ -2,7 +2,7 @@
  * Copyright(C) 2022 Marvell.
  */
 
-#include "cn10k_worker.h"
+#include "cn10k_tx_worker.h"
 
 #define T(name, sz, flags)                                                     \
 	SSO_TX_SEG(cn10k_sso_hws_tx_adptr_enq_seg_##name, sz, flags)
diff --git a/drivers/event/cnxk/tx/cn10k/tx_80_95.c b/drivers/event/cnxk/tx/cn10k/tx_80_95.c
index e400c1546e004..5bf983fb1f6ba 100644
--- a/drivers/event/cnxk/tx/cn10k/tx_80_95.c
+++ b/drivers/event/cnxk/tx/cn10k/tx_80_95.c
@@ -2,7 +2,7 @@
  * Copyright(C) 2022 Marvell.
  */
 
-#include "cn10k_worker.h"
+#include "cn10k_tx_worker.h"
 
 #define T(name, sz, flags) SSO_TX(cn10k_sso_hws_tx_adptr_enq_##name, sz, flags)
 
diff --git a/drivers/event/cnxk/tx/cn10k/tx_80_95_seg.c b/drivers/event/cnxk/tx/cn10k/tx_80_95_seg.c
index 21a76b59fbf34..9344625150620 100644
--- a/drivers/event/cnxk/tx/cn10k/tx_80_95_seg.c
+++ b/drivers/event/cnxk/tx/cn10k/tx_80_95_seg.c
@@ -2,7 +2,7 @@
  * Copyright(C) 2022 Marvell.
  */
 
-#include "cn10k_worker.h"
+#include "cn10k_tx_worker.h"
 
 #define T(name, sz, flags)                                                     \
 	SSO_TX_SEG(cn10k_sso_hws_tx_adptr_enq_seg_##name, sz, flags)
diff --git a/drivers/event/cnxk/tx/cn10k/tx_96_111.c b/drivers/event/cnxk/tx/cn10k/tx_96_111.c
index 6679b9aa28af8..88fb5cb65310a 100644
--- a/drivers/event/cnxk/tx/cn10k/tx_96_111.c
+++ b/drivers/event/cnxk/tx/cn10k/tx_96_111.c
@@ -2,7 +2,7 @@
  * Copyright(C) 2022 Marvell.
  */
 
-#include "cn10k_worker.h"
+#include "cn10k_tx_worker.h"
 
 #define T(name, sz, flags) SSO_TX(cn10k_sso_hws_tx_adptr_enq_##name, sz, flags)
 
diff --git a/drivers/event/cnxk/tx/cn10k/tx_96_111_seg.c b/drivers/event/cnxk/tx/cn10k/tx_96_111_seg.c
index 9430a92fc3398..e9ca2f19d29df 100644
--- a/drivers/event/cnxk/tx/cn10k/tx_96_111_seg.c
+++ b/drivers/event/cnxk/tx/cn10k/tx_96_111_seg.c
@@ -2,7 +2,7 @@
  * Copyright(C) 2022 Marvell.
  */
 
-#include "cn10k_worker.h"
+#include "cn10k_tx_worker.h"
 
 #define T(name, sz, flags)                                                     \
 	SSO_TX_SEG(cn10k_sso_hws_tx_adptr_enq_seg_##name, sz, flags)
-- 
2.25.1

